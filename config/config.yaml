# 普通用户配置专区(怎么改都行)
API_token: "API" # 你的API token，没有自己上WeLM官网https://docs.qq.com/form/page/DUW1YVVJNbHpzV2No#/fill-detail申请
bot_name: "NAME" # 里面填你机器人角色昵称or自设之类的?

# 模型参数调试专区(最好不碰,参数已由作者设定完成)
model: "xl" # 要使用的模型名称，当前支持的模型名称有medium、 large 和 xl。
max_tokens: "128"  # 最多生成的token个数, 默认值 128。
max_tokens_xx: "256" # 续写指令最多生成的token个数，默认值 256。
temperature: "0.85" # 默认值 0.85，更高的temperature意味着模型具备更多的可能性。对于更有创造性的应用，可以尝试0.85以上，而对于有明确答案的应用，可以尝试0（argmax采样）。 建议改变这个值或top_p，但不要同时改变。
top_p: "0.95" # 默认值 0.95，来源于nucleus sampling，采用的是累计概率的方式。
top_k: "50" # 默认值50，从概率分布中依据概率最大选择k个单词，建议不要过小导致模型能选择的词汇少。
n: "2" # 默认值 2, 大于0, 小于等于12 返回的序列的个数
stop: "\n" # 默认值\n，默认停止符号。当模型当前生成的字符为stop中的任何一个字符时，会停止生成。
twstop: "。" # 默认值"。" ，提问指令停止符号。当模型当前生成的字符为stop中的任何一个字符时，会停止生成。

# 插件调试专区(开发人员专用,更改后可能会寄)
version: "4.6"
dhcmdstart: "welm" # 对话指令开头。
wdcmdstart: "提问" # 问答指令开头。
xxcmdstart: "续写" # 续写指令开头。
lxdhcmdstart: "lxdh" # 连续对话指令开头
ydlicmdstart: "阅读理解" # 阅读理解指令开头

dhreplystart: "(由WeLM回答)" # 对话指令回复开头备注, 不用与其他ai区分时可留空。
wdreplystart: "(由WeLM提问)" # 问答指令回复开头备注, 不用与其他ai区分时可留空。
xxreplystart: "(由WeLM续写)" # 续写指令回复开头备注, 不用与其他ai区分时可留空。
lxdhreplystart: "(由WeLM回答(连续对话))" # 连续对话指令回复开头备注, 不用与其他ai区分时可留空。
ydljreplystart: "(由welm阅读理解)"